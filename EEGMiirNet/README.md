# Convolutional Neural Network for Music Imagery Information Retrieval
## Intro
This project directory contains my Pytorch CNN model, training scripts, preprocessing scripts, and data for my experiments on the OpenMIIR public EEG dataset. The OpenMIIR dataset contains electroencephalography recordings taken from nine participants listening to and imagining twelve well-known musical pieces. Of these twelve pieces, four are instrumental, four are lyrical, and the last four are lyrics-omitted versions of the four lyrical pieces. The goal of this project was to build and train a CNN model that could identify which of the musical pieces was being listened to from a given raw EEG data sample taken from a random participant.
## Approach 
Due to the difficulty of EEG data and the limited sample size of my dataset, my model had a strong tendency to overfit. To combat this, my CNN model uses a fairly shallow network along with steep L1 regularization and dropout. To assess the model's ability to generalize to unseen data, I employed a Leave-One-Out Cross-Validation scheme across participants. In this approach, my model was trained on the data of eight participants and evaluated on the data from the nineth participant. By measuring average performance across all nine folds, each in which a different participant’s data comprises the evaluation set, I hoped to take advantage of the significant overlap (low variation) across different training sets and complete independence (high variation) across different evaluation set. The dataset for each condition (lyrics/non-lyrics) consisted of 360 samples: five recordings by eight songs by nine participants. I trained the model for 50 epochs at a learning rate of 0.01 with a training and evaluation batch size of 40. Cross Entropy Loss was used with the Adam optimizer.
## Results
The average cross-fold accuracy after training in the lyrics condition was 37.0% while the average for the lyrics-omitted condition was 38.7%. Expectedly, not all folds were equal. Some folds in the LOOCV scheme achieved over 50% average accuracy, whereas the worst fold achieved only 28.75% accuracy. Given that in each fold only one participant's data comprised the evaluation set, this variance is expected consideraing the high variation in EEG data across different individuals, especially for a task as personalized as music perception. The figures below show confusion matrices for the different conditions where the eight classes represent the eight songs being predicted.
#### Lyrics Condition: 
![confusion_matrix_run1](https://github.com/user-attachments/assets/8cca3fe5-cd51-4763-aa15-f47a61627895)
#### Lyrics-Omitted Condition: 
![confusion_matrix_run1_no_lyrics](https://github.com/user-attachments/assets/7f5a2727-0453-46db-913c-59e543a2d121)
## Citations:
* J. S. Snyder and E. W. Large. Gamma-band activity reflects the metric structure of rhythmic tone sequences. Cognitive Brain Research, 24:117–126, 2005.
* Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M. Gordon, Chou P. Hung, Brent J. Lance. EEGNet: A Compact Convolutional Network for EEG-based Brain-  Computer Interfaces. J. Neural Eng. 15 056013, 2018
* S. Stober, A Stemin, A.M. Owen, and lA. Grahn, "Towards music imagery information retrieval: Introducing the OpenMIIR dataset of EEG recordings from music perception and imagination;' in 16th International Society for Music Information Retrieval Conference (ISMIR), 2015, pp. 763- 769.

